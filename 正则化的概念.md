## 正则化

正则化是一个大类，包括L1正则化和L2正则化，L1正则化又称为lasso回归(Lasso Regression)，L2正则化又称为岭回归(Ridge Regression)。L1正则化和L2正则化合用，又被称为Elastic网络回归(Elastic Net Regression)。

>在数学与计算机科学中，尤其是在机器学习和逆问题领域中，正则化（英语：regularization）是指为解决适定性问题或过拟合而加入额外信息的过程。[维基百科](https://zh.wikipedia.org/wiki/%E6%AD%A3%E5%88%99%E5%8C%96_(%E6%95%B0%E5%AD%A6))


### 岭回归
岭回归原理的概览：在机器学习领域，某个模型在训练数据中表现良好而在测试数据中表现糟糕的现象，称为过拟合（over fit）。岭回归的作用就是缓冲这种过拟合现象，具体而言就是在拟合模型（红色直线）中引入少量偏差（bias）形成新拟合模型（蓝色直线），并以此为代价减少拟合模型的方差（variance），使新拟合模型在测试数据中的表现更好。

https://mp.weixin.qq.com/s/IK1CSugA7gdcGNp0pCPcWQ




### Lasso回归
https://mp.weixin.qq.com/s/VCJWBzDS0KNrj_JAu_EmkA

Lasso回归同样是通过残差平方和与惩罚项总和确定lasso回归模型，但lasso回归的惩罚项为λ  x (斜率的绝对值)。其λ值的取值范围为[0，+∞)，由交叉验证得出最佳λ值。



在仅含有两个样本的训练数据集中，lasso回归模型满足（残差平方和 + λ x 斜率绝对值）之和最小。lasso回归可减少创建模型中的参数（如减少无关变量的参数个数）。

当λ=0时，lasso回归与最小二乘法直线回归一致。
当λ＞0时，随着λ的增大，lasso回归中直线的斜率逐渐减小，直至为0。


如果模型中含有较多的无关变量时，因lasso回归可以将无关变量排除，故lasso回归比岭回归模型更优，其在不同数据集中的方差更小。

相反，如果模型中大多数变量为相关变量时，因岭回归不会误删一些变量，故岭回归比lasso回归模型更优，其在不同数据集中的方差更小。

#### 维基百科解释LASSO
在统计学和机器学习中，Lasso算法（英语：least absolute shrinkage and selection operator，又译最小绝对值收敛和选择算子、套索算法）是一种同时进行特征选择和正则化（数学）的回归分析方法，旨在增强统计模型的预测准确性和可解释性，最初由斯坦福大学统计学教授Robert Tibshirani于1996年基于Leo Breiman的非负参数推断(Nonnegative Garrote, NNG)提出。Lasso算法最初用于计算最小二乘法模型，这个简单的算法揭示了很多估计量的重要性质，如估计量与岭回归（Ridge regression，也叫吉洪诺夫正则化）和最佳子集选择的关系，Lasso系数估计值(estimate)和软阈值（soft thresholding）之间的联系。它也揭示了当协变量共线时，Lasso系数估计值不一定唯一（类似标准线性回归）。

虽然最早是为应用最小二乘法而定义的算法，lasso正则化可以简单直接地拓展应用于许多统计学模型上，包括广义线性模型，广义估计方程，成比例灾难模型和M-估计。Lasso选择子集的能力依赖于限制条件的形式并且有多种表现形式，包括几何学，贝叶斯统计，和凸分析。

Lasso算法与基追踪降噪联系紧密。
https://zh.wikipedia.org/wiki/Lasso%E7%AE%97%E6%B3%95
